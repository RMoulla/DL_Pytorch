{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RMoulla/DL_Pytorch/blob/main/TP_GNN_GAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# TP : Réseaux de neurones graphiques à base d'Attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "Dans ce TP, nous nous proposons d'étudier le dataset Cora. Celui-ci est composé de 2708 papiers scientifiques répartis en sept classes. L'objectif de ce TP est de construire un modèle permettant de prédire la classe à laquelle appartient un noeud (papier scientifique).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82DqiWzUG53f",
        "outputId": "c3009ad1-b85a-4aa3-ac76-8061a9186b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWPkJjPAfVNW"
      },
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.utils import to_networkx\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqUnYT5qUZYh"
      },
      "source": [
        "## Analyse du dataset Cora\n",
        "\n",
        "On se propose ici d'explorer le dataset Cora. Il est fortement recommandé de compléter cette analyse exploratoire (visualiser le graphe, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset = Planetoid(root='~/somewhere/Cora', name='Cora')\n",
        "\n",
        "data = dataset[0]\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('======================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
        "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {data.contains_self_loops()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTeFQyPUNE4A",
        "outputId": "4f0d5404-8a5f-4203-a1ca-52e2a776abb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Cora():\n",
            "======================\n",
            "Number of graphs: 1\n",
            "Number of features: 1433\n",
            "Number of classes: 7\n",
            "Number of nodes: 2708\n",
            "Number of edges: 10556\n",
            "Average node degree: 3.90\n",
            "Number of training nodes: 140\n",
            "Training node label rate: 0.05\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n",
            "Done!\n",
            "/tmp/ipython-input-3-383367433.py:15: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
            "  print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
            "/tmp/ipython-input-3-383367433.py:16: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n",
            "  print(f'Contains self-loops: {data.contains_self_loops()}')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX25Y1CrYmgN"
      },
      "source": [
        "## Question 1 : construire un modèle à base de GAT qui permet de prédire la classe à laquelle appartient un noeud.\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GATNet(nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(GATNet, self).__init__()\n",
        "        self.conv1 = GATConv(num_features, 8, heads=8, dropout=0.6)\n",
        "        self.conv2 = GATConv(8 * 8, num_classes, heads=1, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Créer le modèle\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GATNet(dataset.num_node_features, dataset.num_classes).to(device)\n",
        "data = dataset[0].to(device)\n",
        "\n",
        "# Fonction de perte et optimiseur\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "# Fonction d'entraînement\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss\n",
        "\n",
        "# Fonction de test\n",
        "def test():\n",
        "    model.eval()\n",
        "    logits, accs = model(data), []\n",
        "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "# Boucle d'entraînement\n",
        "for epoch in range(200):\n",
        "    loss = train()\n",
        "    train_acc, val_acc, test_acc = test()\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
        "          f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hgsdf9ykbGKv",
        "outputId": "ffc50cf8-318b-45f9-8cdd-0ac6326c7bc1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Loss: 1.9993, Train: 0.5143, Val: 0.3600, Test: 0.3420\n",
            "Epoch: 001, Loss: 1.9196, Train: 0.7786, Val: 0.5300, Test: 0.5020\n",
            "Epoch: 002, Loss: 1.8545, Train: 0.8357, Val: 0.5960, Test: 0.5740\n",
            "Epoch: 003, Loss: 1.7803, Train: 0.8714, Val: 0.6480, Test: 0.6200\n",
            "Epoch: 004, Loss: 1.6487, Train: 0.8786, Val: 0.6860, Test: 0.6670\n",
            "Epoch: 005, Loss: 1.5234, Train: 0.9000, Val: 0.7080, Test: 0.6960\n",
            "Epoch: 006, Loss: 1.4656, Train: 0.9143, Val: 0.7260, Test: 0.7210\n",
            "Epoch: 007, Loss: 1.3824, Train: 0.9357, Val: 0.7400, Test: 0.7300\n",
            "Epoch: 008, Loss: 1.3965, Train: 0.9500, Val: 0.7360, Test: 0.7290\n",
            "Epoch: 009, Loss: 1.4369, Train: 0.9500, Val: 0.7300, Test: 0.7300\n",
            "Epoch: 010, Loss: 1.3526, Train: 0.9500, Val: 0.7320, Test: 0.7410\n",
            "Epoch: 011, Loss: 1.2129, Train: 0.9500, Val: 0.7260, Test: 0.7460\n",
            "Epoch: 012, Loss: 1.2498, Train: 0.9571, Val: 0.7340, Test: 0.7460\n",
            "Epoch: 013, Loss: 1.1285, Train: 0.9571, Val: 0.7380, Test: 0.7470\n",
            "Epoch: 014, Loss: 1.1462, Train: 0.9643, Val: 0.7400, Test: 0.7460\n",
            "Epoch: 015, Loss: 1.0986, Train: 0.9643, Val: 0.7340, Test: 0.7480\n",
            "Epoch: 016, Loss: 1.0344, Train: 0.9714, Val: 0.7380, Test: 0.7510\n",
            "Epoch: 017, Loss: 0.9659, Train: 0.9714, Val: 0.7460, Test: 0.7570\n",
            "Epoch: 018, Loss: 1.0037, Train: 0.9714, Val: 0.7480, Test: 0.7520\n",
            "Epoch: 019, Loss: 0.9959, Train: 0.9643, Val: 0.7540, Test: 0.7510\n",
            "Epoch: 020, Loss: 0.9874, Train: 0.9643, Val: 0.7520, Test: 0.7530\n",
            "Epoch: 021, Loss: 0.8651, Train: 0.9643, Val: 0.7440, Test: 0.7500\n",
            "Epoch: 022, Loss: 0.8897, Train: 0.9714, Val: 0.7460, Test: 0.7510\n",
            "Epoch: 023, Loss: 0.9165, Train: 0.9714, Val: 0.7440, Test: 0.7540\n",
            "Epoch: 024, Loss: 0.9397, Train: 0.9857, Val: 0.7440, Test: 0.7560\n",
            "Epoch: 025, Loss: 0.7876, Train: 0.9857, Val: 0.7560, Test: 0.7580\n",
            "Epoch: 026, Loss: 0.8517, Train: 0.9857, Val: 0.7560, Test: 0.7590\n",
            "Epoch: 027, Loss: 0.8212, Train: 0.9857, Val: 0.7580, Test: 0.7630\n",
            "Epoch: 028, Loss: 0.9505, Train: 0.9857, Val: 0.7600, Test: 0.7680\n",
            "Epoch: 029, Loss: 0.8569, Train: 0.9857, Val: 0.7600, Test: 0.7730\n",
            "Epoch: 030, Loss: 0.7183, Train: 0.9857, Val: 0.7620, Test: 0.7740\n",
            "Epoch: 031, Loss: 0.7248, Train: 0.9929, Val: 0.7620, Test: 0.7750\n",
            "Epoch: 032, Loss: 0.6801, Train: 0.9929, Val: 0.7620, Test: 0.7780\n",
            "Epoch: 033, Loss: 0.7677, Train: 0.9929, Val: 0.7640, Test: 0.7770\n",
            "Epoch: 034, Loss: 0.7040, Train: 0.9929, Val: 0.7640, Test: 0.7770\n",
            "Epoch: 035, Loss: 0.5820, Train: 0.9929, Val: 0.7600, Test: 0.7770\n",
            "Epoch: 036, Loss: 0.7097, Train: 0.9929, Val: 0.7580, Test: 0.7770\n",
            "Epoch: 037, Loss: 0.6193, Train: 0.9929, Val: 0.7580, Test: 0.7770\n",
            "Epoch: 038, Loss: 0.6880, Train: 0.9929, Val: 0.7540, Test: 0.7780\n",
            "Epoch: 039, Loss: 0.7538, Train: 0.9929, Val: 0.7560, Test: 0.7810\n",
            "Epoch: 040, Loss: 0.6305, Train: 0.9929, Val: 0.7560, Test: 0.7800\n",
            "Epoch: 041, Loss: 0.6424, Train: 0.9929, Val: 0.7520, Test: 0.7800\n",
            "Epoch: 042, Loss: 0.5682, Train: 0.9929, Val: 0.7500, Test: 0.7790\n",
            "Epoch: 043, Loss: 0.5034, Train: 0.9929, Val: 0.7520, Test: 0.7780\n",
            "Epoch: 044, Loss: 0.6042, Train: 0.9929, Val: 0.7480, Test: 0.7770\n",
            "Epoch: 045, Loss: 0.6387, Train: 0.9929, Val: 0.7460, Test: 0.7790\n",
            "Epoch: 046, Loss: 0.6408, Train: 0.9929, Val: 0.7500, Test: 0.7780\n",
            "Epoch: 047, Loss: 0.5822, Train: 0.9929, Val: 0.7520, Test: 0.7770\n",
            "Epoch: 048, Loss: 0.5961, Train: 0.9929, Val: 0.7500, Test: 0.7760\n",
            "Epoch: 049, Loss: 0.5085, Train: 0.9929, Val: 0.7500, Test: 0.7760\n",
            "Epoch: 050, Loss: 0.5487, Train: 0.9929, Val: 0.7500, Test: 0.7770\n",
            "Epoch: 051, Loss: 0.6599, Train: 0.9929, Val: 0.7500, Test: 0.7780\n",
            "Epoch: 052, Loss: 0.5929, Train: 1.0000, Val: 0.7540, Test: 0.7800\n",
            "Epoch: 053, Loss: 0.4985, Train: 1.0000, Val: 0.7540, Test: 0.7800\n",
            "Epoch: 054, Loss: 0.5774, Train: 1.0000, Val: 0.7540, Test: 0.7820\n",
            "Epoch: 055, Loss: 0.4599, Train: 1.0000, Val: 0.7540, Test: 0.7820\n",
            "Epoch: 056, Loss: 0.5080, Train: 1.0000, Val: 0.7500, Test: 0.7810\n",
            "Epoch: 057, Loss: 0.4622, Train: 1.0000, Val: 0.7520, Test: 0.7810\n",
            "Epoch: 058, Loss: 0.5128, Train: 1.0000, Val: 0.7520, Test: 0.7800\n",
            "Epoch: 059, Loss: 0.5955, Train: 0.9929, Val: 0.7520, Test: 0.7770\n",
            "Epoch: 060, Loss: 0.5601, Train: 0.9929, Val: 0.7540, Test: 0.7760\n",
            "Epoch: 061, Loss: 0.5919, Train: 0.9929, Val: 0.7540, Test: 0.7770\n",
            "Epoch: 062, Loss: 0.6448, Train: 1.0000, Val: 0.7520, Test: 0.7780\n",
            "Epoch: 063, Loss: 0.6240, Train: 1.0000, Val: 0.7560, Test: 0.7790\n",
            "Epoch: 064, Loss: 0.5716, Train: 1.0000, Val: 0.7600, Test: 0.7790\n",
            "Epoch: 065, Loss: 0.5779, Train: 1.0000, Val: 0.7580, Test: 0.7810\n",
            "Epoch: 066, Loss: 0.5082, Train: 1.0000, Val: 0.7620, Test: 0.7810\n",
            "Epoch: 067, Loss: 0.5797, Train: 1.0000, Val: 0.7620, Test: 0.7820\n",
            "Epoch: 068, Loss: 0.4723, Train: 1.0000, Val: 0.7620, Test: 0.7830\n",
            "Epoch: 069, Loss: 0.4794, Train: 1.0000, Val: 0.7600, Test: 0.7850\n",
            "Epoch: 070, Loss: 0.5045, Train: 1.0000, Val: 0.7620, Test: 0.7860\n",
            "Epoch: 071, Loss: 0.6049, Train: 1.0000, Val: 0.7640, Test: 0.7860\n",
            "Epoch: 072, Loss: 0.4387, Train: 1.0000, Val: 0.7660, Test: 0.7870\n",
            "Epoch: 073, Loss: 0.4206, Train: 1.0000, Val: 0.7660, Test: 0.7870\n",
            "Epoch: 074, Loss: 0.4943, Train: 1.0000, Val: 0.7640, Test: 0.7870\n",
            "Epoch: 075, Loss: 0.4749, Train: 1.0000, Val: 0.7620, Test: 0.7860\n",
            "Epoch: 076, Loss: 0.5142, Train: 1.0000, Val: 0.7580, Test: 0.7820\n",
            "Epoch: 077, Loss: 0.4834, Train: 1.0000, Val: 0.7520, Test: 0.7790\n",
            "Epoch: 078, Loss: 0.3733, Train: 1.0000, Val: 0.7520, Test: 0.7790\n",
            "Epoch: 079, Loss: 0.5940, Train: 1.0000, Val: 0.7480, Test: 0.7790\n",
            "Epoch: 080, Loss: 0.4596, Train: 1.0000, Val: 0.7480, Test: 0.7780\n",
            "Epoch: 081, Loss: 0.5070, Train: 1.0000, Val: 0.7480, Test: 0.7770\n",
            "Epoch: 082, Loss: 0.3728, Train: 1.0000, Val: 0.7480, Test: 0.7760\n",
            "Epoch: 083, Loss: 0.3873, Train: 1.0000, Val: 0.7460, Test: 0.7750\n",
            "Epoch: 084, Loss: 0.5356, Train: 1.0000, Val: 0.7440, Test: 0.7730\n",
            "Epoch: 085, Loss: 0.6540, Train: 1.0000, Val: 0.7420, Test: 0.7750\n",
            "Epoch: 086, Loss: 0.4326, Train: 1.0000, Val: 0.7420, Test: 0.7740\n",
            "Epoch: 087, Loss: 0.5875, Train: 1.0000, Val: 0.7440, Test: 0.7730\n",
            "Epoch: 088, Loss: 0.4964, Train: 1.0000, Val: 0.7480, Test: 0.7720\n",
            "Epoch: 089, Loss: 0.5498, Train: 1.0000, Val: 0.7460, Test: 0.7720\n",
            "Epoch: 090, Loss: 0.4597, Train: 1.0000, Val: 0.7420, Test: 0.7720\n",
            "Epoch: 091, Loss: 0.5234, Train: 1.0000, Val: 0.7440, Test: 0.7730\n",
            "Epoch: 092, Loss: 0.5150, Train: 1.0000, Val: 0.7440, Test: 0.7760\n",
            "Epoch: 093, Loss: 0.4533, Train: 1.0000, Val: 0.7480, Test: 0.7760\n",
            "Epoch: 094, Loss: 0.4293, Train: 1.0000, Val: 0.7500, Test: 0.7780\n",
            "Epoch: 095, Loss: 0.4976, Train: 1.0000, Val: 0.7480, Test: 0.7790\n",
            "Epoch: 096, Loss: 0.4547, Train: 1.0000, Val: 0.7480, Test: 0.7790\n",
            "Epoch: 097, Loss: 0.4194, Train: 1.0000, Val: 0.7480, Test: 0.7790\n",
            "Epoch: 098, Loss: 0.4499, Train: 1.0000, Val: 0.7460, Test: 0.7790\n",
            "Epoch: 099, Loss: 0.4941, Train: 0.9929, Val: 0.7460, Test: 0.7780\n",
            "Epoch: 100, Loss: 0.4439, Train: 1.0000, Val: 0.7460, Test: 0.7790\n",
            "Epoch: 101, Loss: 0.5195, Train: 0.9929, Val: 0.7460, Test: 0.7770\n",
            "Epoch: 102, Loss: 0.6012, Train: 1.0000, Val: 0.7500, Test: 0.7790\n",
            "Epoch: 103, Loss: 0.5340, Train: 1.0000, Val: 0.7560, Test: 0.7820\n",
            "Epoch: 104, Loss: 0.4747, Train: 1.0000, Val: 0.7560, Test: 0.7820\n",
            "Epoch: 105, Loss: 0.3724, Train: 1.0000, Val: 0.7560, Test: 0.7830\n",
            "Epoch: 106, Loss: 0.4484, Train: 1.0000, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 107, Loss: 0.5588, Train: 1.0000, Val: 0.7600, Test: 0.7850\n",
            "Epoch: 108, Loss: 0.4779, Train: 1.0000, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 109, Loss: 0.4947, Train: 1.0000, Val: 0.7580, Test: 0.7870\n",
            "Epoch: 110, Loss: 0.2990, Train: 1.0000, Val: 0.7580, Test: 0.7880\n",
            "Epoch: 111, Loss: 0.3641, Train: 1.0000, Val: 0.7600, Test: 0.7880\n",
            "Epoch: 112, Loss: 0.3668, Train: 1.0000, Val: 0.7600, Test: 0.7890\n",
            "Epoch: 113, Loss: 0.4815, Train: 1.0000, Val: 0.7580, Test: 0.7900\n",
            "Epoch: 114, Loss: 0.4600, Train: 1.0000, Val: 0.7600, Test: 0.7890\n",
            "Epoch: 115, Loss: 0.5142, Train: 1.0000, Val: 0.7620, Test: 0.7890\n",
            "Epoch: 116, Loss: 0.3334, Train: 1.0000, Val: 0.7620, Test: 0.7890\n",
            "Epoch: 117, Loss: 0.4997, Train: 1.0000, Val: 0.7620, Test: 0.7900\n",
            "Epoch: 118, Loss: 0.4407, Train: 1.0000, Val: 0.7620, Test: 0.7910\n",
            "Epoch: 119, Loss: 0.3802, Train: 1.0000, Val: 0.7620, Test: 0.7930\n",
            "Epoch: 120, Loss: 0.3786, Train: 1.0000, Val: 0.7600, Test: 0.7930\n",
            "Epoch: 121, Loss: 0.3913, Train: 1.0000, Val: 0.7620, Test: 0.7920\n",
            "Epoch: 122, Loss: 0.4417, Train: 1.0000, Val: 0.7600, Test: 0.7930\n",
            "Epoch: 123, Loss: 0.3831, Train: 1.0000, Val: 0.7600, Test: 0.7920\n",
            "Epoch: 124, Loss: 0.4641, Train: 1.0000, Val: 0.7560, Test: 0.7890\n",
            "Epoch: 125, Loss: 0.4830, Train: 1.0000, Val: 0.7540, Test: 0.7880\n",
            "Epoch: 126, Loss: 0.4975, Train: 1.0000, Val: 0.7460, Test: 0.7840\n",
            "Epoch: 127, Loss: 0.4099, Train: 1.0000, Val: 0.7440, Test: 0.7800\n",
            "Epoch: 128, Loss: 0.4915, Train: 1.0000, Val: 0.7460, Test: 0.7810\n",
            "Epoch: 129, Loss: 0.3299, Train: 1.0000, Val: 0.7440, Test: 0.7790\n",
            "Epoch: 130, Loss: 0.3165, Train: 1.0000, Val: 0.7420, Test: 0.7780\n",
            "Epoch: 131, Loss: 0.3451, Train: 1.0000, Val: 0.7420, Test: 0.7790\n",
            "Epoch: 132, Loss: 0.4718, Train: 1.0000, Val: 0.7420, Test: 0.7790\n",
            "Epoch: 133, Loss: 0.4495, Train: 1.0000, Val: 0.7420, Test: 0.7790\n",
            "Epoch: 134, Loss: 0.3946, Train: 1.0000, Val: 0.7440, Test: 0.7810\n",
            "Epoch: 135, Loss: 0.3059, Train: 1.0000, Val: 0.7480, Test: 0.7830\n",
            "Epoch: 136, Loss: 0.3822, Train: 1.0000, Val: 0.7480, Test: 0.7840\n",
            "Epoch: 137, Loss: 0.4570, Train: 1.0000, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 138, Loss: 0.4435, Train: 1.0000, Val: 0.7540, Test: 0.7850\n",
            "Epoch: 139, Loss: 0.3316, Train: 1.0000, Val: 0.7540, Test: 0.7870\n",
            "Epoch: 140, Loss: 0.4738, Train: 1.0000, Val: 0.7560, Test: 0.7860\n",
            "Epoch: 141, Loss: 0.3499, Train: 1.0000, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 142, Loss: 0.4449, Train: 1.0000, Val: 0.7540, Test: 0.7880\n",
            "Epoch: 143, Loss: 0.5061, Train: 1.0000, Val: 0.7560, Test: 0.7870\n",
            "Epoch: 144, Loss: 0.4400, Train: 1.0000, Val: 0.7680, Test: 0.7930\n",
            "Epoch: 145, Loss: 0.3364, Train: 1.0000, Val: 0.7680, Test: 0.7950\n",
            "Epoch: 146, Loss: 0.4005, Train: 1.0000, Val: 0.7760, Test: 0.7970\n",
            "Epoch: 147, Loss: 0.4374, Train: 1.0000, Val: 0.7760, Test: 0.7990\n",
            "Epoch: 148, Loss: 0.4977, Train: 1.0000, Val: 0.7820, Test: 0.8010\n",
            "Epoch: 149, Loss: 0.4164, Train: 1.0000, Val: 0.7820, Test: 0.8030\n",
            "Epoch: 150, Loss: 0.4112, Train: 1.0000, Val: 0.7820, Test: 0.7990\n",
            "Epoch: 151, Loss: 0.4445, Train: 1.0000, Val: 0.7820, Test: 0.8010\n",
            "Epoch: 152, Loss: 0.3855, Train: 1.0000, Val: 0.7840, Test: 0.8030\n",
            "Epoch: 153, Loss: 0.3308, Train: 1.0000, Val: 0.7840, Test: 0.8020\n",
            "Epoch: 154, Loss: 0.3988, Train: 1.0000, Val: 0.7840, Test: 0.8010\n",
            "Epoch: 155, Loss: 0.4537, Train: 1.0000, Val: 0.7860, Test: 0.7990\n",
            "Epoch: 156, Loss: 0.4585, Train: 1.0000, Val: 0.7800, Test: 0.7980\n",
            "Epoch: 157, Loss: 0.5006, Train: 1.0000, Val: 0.7780, Test: 0.7970\n",
            "Epoch: 158, Loss: 0.4441, Train: 1.0000, Val: 0.7760, Test: 0.7940\n",
            "Epoch: 159, Loss: 0.4184, Train: 1.0000, Val: 0.7720, Test: 0.7910\n",
            "Epoch: 160, Loss: 0.4063, Train: 1.0000, Val: 0.7740, Test: 0.7900\n",
            "Epoch: 161, Loss: 0.2967, Train: 1.0000, Val: 0.7760, Test: 0.7920\n",
            "Epoch: 162, Loss: 0.4335, Train: 1.0000, Val: 0.7760, Test: 0.7910\n",
            "Epoch: 163, Loss: 0.2921, Train: 1.0000, Val: 0.7720, Test: 0.7890\n",
            "Epoch: 164, Loss: 0.3485, Train: 1.0000, Val: 0.7720, Test: 0.7860\n",
            "Epoch: 165, Loss: 0.4390, Train: 1.0000, Val: 0.7700, Test: 0.7880\n",
            "Epoch: 166, Loss: 0.3450, Train: 1.0000, Val: 0.7700, Test: 0.7890\n",
            "Epoch: 167, Loss: 0.3897, Train: 1.0000, Val: 0.7680, Test: 0.7890\n",
            "Epoch: 168, Loss: 0.3968, Train: 1.0000, Val: 0.7680, Test: 0.7900\n",
            "Epoch: 169, Loss: 0.4365, Train: 1.0000, Val: 0.7700, Test: 0.7890\n",
            "Epoch: 170, Loss: 0.3603, Train: 1.0000, Val: 0.7700, Test: 0.7890\n",
            "Epoch: 171, Loss: 0.4841, Train: 1.0000, Val: 0.7640, Test: 0.7900\n",
            "Epoch: 172, Loss: 0.4138, Train: 1.0000, Val: 0.7620, Test: 0.7890\n",
            "Epoch: 173, Loss: 0.4100, Train: 1.0000, Val: 0.7620, Test: 0.7900\n",
            "Epoch: 174, Loss: 0.4556, Train: 1.0000, Val: 0.7620, Test: 0.7900\n",
            "Epoch: 175, Loss: 0.3724, Train: 1.0000, Val: 0.7620, Test: 0.7900\n",
            "Epoch: 176, Loss: 0.4177, Train: 1.0000, Val: 0.7600, Test: 0.7910\n",
            "Epoch: 177, Loss: 0.4933, Train: 1.0000, Val: 0.7600, Test: 0.7890\n",
            "Epoch: 178, Loss: 0.3808, Train: 1.0000, Val: 0.7580, Test: 0.7870\n",
            "Epoch: 179, Loss: 0.3799, Train: 1.0000, Val: 0.7560, Test: 0.7850\n",
            "Epoch: 180, Loss: 0.3990, Train: 1.0000, Val: 0.7520, Test: 0.7830\n",
            "Epoch: 181, Loss: 0.4169, Train: 1.0000, Val: 0.7500, Test: 0.7830\n",
            "Epoch: 182, Loss: 0.3582, Train: 1.0000, Val: 0.7500, Test: 0.7830\n",
            "Epoch: 183, Loss: 0.4461, Train: 1.0000, Val: 0.7500, Test: 0.7840\n",
            "Epoch: 184, Loss: 0.3537, Train: 1.0000, Val: 0.7500, Test: 0.7850\n",
            "Epoch: 185, Loss: 0.3695, Train: 1.0000, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 186, Loss: 0.5423, Train: 1.0000, Val: 0.7520, Test: 0.7850\n",
            "Epoch: 187, Loss: 0.3906, Train: 1.0000, Val: 0.7540, Test: 0.7850\n",
            "Epoch: 188, Loss: 0.5291, Train: 1.0000, Val: 0.7620, Test: 0.7860\n",
            "Epoch: 189, Loss: 0.3970, Train: 1.0000, Val: 0.7620, Test: 0.7850\n",
            "Epoch: 190, Loss: 0.3410, Train: 1.0000, Val: 0.7620, Test: 0.7860\n",
            "Epoch: 191, Loss: 0.4028, Train: 1.0000, Val: 0.7620, Test: 0.7860\n",
            "Epoch: 192, Loss: 0.4516, Train: 1.0000, Val: 0.7600, Test: 0.7860\n",
            "Epoch: 193, Loss: 0.3238, Train: 1.0000, Val: 0.7620, Test: 0.7890\n",
            "Epoch: 194, Loss: 0.3264, Train: 1.0000, Val: 0.7620, Test: 0.7880\n",
            "Epoch: 195, Loss: 0.3561, Train: 1.0000, Val: 0.7620, Test: 0.7880\n",
            "Epoch: 196, Loss: 0.3613, Train: 1.0000, Val: 0.7660, Test: 0.7860\n",
            "Epoch: 197, Loss: 0.3481, Train: 1.0000, Val: 0.7660, Test: 0.7880\n",
            "Epoch: 198, Loss: 0.4675, Train: 1.0000, Val: 0.7680, Test: 0.7900\n",
            "Epoch: 199, Loss: 0.3187, Train: 1.0000, Val: 0.7700, Test: 0.7920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vvzaQzIMdAzI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}